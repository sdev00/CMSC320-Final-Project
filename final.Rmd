---
title: "CMSC320 Final Project"
output: html_document
---

### Project by Nikos Koutsoheras, Sahil Dev, and Vivekjyoti Banerjee

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(reticulate)
library(rgeolocate)
library(stringr)
library(dplyr)
library(knitr)
library(kableExtra)

matplotlib <- import("matplotlib")
pandas <- import("pandas")
matplotlib$use("Agg", force = TRUE)
sklearn <- import("sklearn")
```

## Introduction
The days when systems administrators could rely on the obscurity of their servers as protection from poor configuration are behind us. Any machine connected to the Internet will be invariably subjected to a background radiation of attackers trying a variety of known software vulnerabilities and common security pitfalls in the hopes of stumbling upon a machine they can control. Only by understanding how these attacks work can we adequately protect our computers from them, and only by observing attackers can we understand the patterns governing their behaviors. 

Possibly the most effective way to observe an attacker is to watch them work on a machine we control. To this end, we can construct a series of honeypots: servers made vulnerable to exploitation exposed on the open web. Attackers can freely connect to these honeypots, which will intentionally succumb to any exploit they attempt to use to gain access. Once inside, attackers can install any variety of malicious software they please and search for valuable data anywhere on the machine. When they leave, we have the opportunity to inspect their actions as well as any software they installed at our leisure.

The actual observation is carried out with a program known as a man-in-the-middle, which serves as the interface between the honeypot and the Internet, passing data seamlessly between the two while recording all that it relays.
(In reality, our honeypots are containers (a lightweight alternative to virtual machines that relies on services provided by the host operating system, rather than a virtualization of its own) running on the MITM server, but it may be easier to think of them as separate physical machines - they certainly could be.)

We analyze in this study the activity recorded on a configuration of five such honeypots managed by a single MITM server over the course of roughly two months.

## Ingestion
The data consists of 60,478 login attempts and 1,769 attack sessions split across 3,538 files.
Each most deeply nested folder represents a single instance of a single container. An instance folder will contain, among others, an attempts.txt file and a mitmlog_30*.txt file. We'll let R find them all and load them in.
```{r ingestion}
# locate all of the relevant files
# attempts contains tidy login data: each line contains data about a unique login attempt
attempts <- list.files(pattern = "attempts", recursive = TRUE)
# mitmlogs contains the raw logs generated by the MITM
mitmlogs <- list.files(pattern = "mitmlog", recursive = TRUE)
files <- data.frame(Attempts=attempts, Logs=mitmlogs)

# the binary database used by ip2location
ip_database <- system.file("extdata","GeoLite2-Country.mmdb", package = "rgeolocate")
```

The simpler of the two parsing steps involves the attempts.txt file present in every instance folder. The MITM manager script appended a line to this file every time somebody attempted to log in to the server. A single line contains the time of an attempted login, the IP address of the computer attempting to log in, the method they used (almost exclusively "password"), and the credentials they provided. We extract all of these fields and use the rgeolocate package to obtain the attacker's country of origin (or their VPN's) from their IP address.

Storing an attacker's IP and their country in every row represents a bit of redundancy. Ideally, we would store the IPs in the attempts table and create another table associating IPs with their corresponding countries. Our table isn't even in first normal form!
...But this is 320, not 424. We'll worry about that some other time.

``` {r parse line}
# parses a single line in an attempts file
parseLine <- function(line) {
  # I'm lucky none of the attackers used a semicolon in their password guess; this could've been irritating
  segments <- strsplit(line, ";")
  location <- maxmind(segments[[1]][2], ip_database)
  return(data.frame(Time=segments[[1]][1],
                    Address=segments[[1]][2],
                    Method=segments[[1]][3],
                    Username=segments[[1]][4],
                    Password=segments[[1]][5],
                    CountryCode=location[3][1],
                    CountryName=location[2][1]))
}
```

In addition to the data we've retrieved from the attempts file, we also want to know how long an attacker spent in a honeypot. It would have been trivial to add a variable that kept track of that to the MITM manager script and write the difference to the end of every line in the attempts.txt files when we were designing the honeypot architecture. If we had done that, knowing that we would need it now, accessing the data here would have been as simple as adding another field to the data frame we created in the parseLine method.
We lacked that insight at the time, so, as punishment, we must now parse the data we need from the larger log file.

Fortunately, the MITM log contains an entry for a successful authentication, which we can locate by the unique "[LXC-Auth]" marker that will appear once in every file. We also know that the container is shut down, destroyed, and rebuilt from scratch when the attacker leaves (or is forced out due to inactivity). As such, the time associated with the last line in the log entry represents the last point at which the attacker was active in the honeypot.

The timestamp attached to the authentication line in the MITM log will be a few milliseconds off from the corresponding entry in attempts.txt. To resolve the pair, we say that the authentication entry observed in the MITM log represents the same attempt as the line in the attempts file with the closest timestamp. At first glance, it might seem that associating the entries in the two files is needlessly complex. Since every IP address except for the attacker's is blacklisted from the container after a successful authentication to prevent multiple attackers from using it at once, and since the log ends when the attacker leaves, it should stand to reason that the last entry in the attempts file be the one that was allowed inside. Unfortunately, that won't work because attackers will occasionally try another username/password combination after gaining access. We speculate that the scripts the attackers use are programmed to try all of their credentials as quickly as possible, rather than wait for the server's response after each one.

``` {r parse folder}
# parses a single attempts/mitmlog pair
parseFile <- function(file) {
  # join all of the one-row data frames produced by calling parseLine on every line in the attempts file into a single frame
  file_df <- rbindlist(lapply(readLines(con = file[1]), parseLine))
  
  log <- readLines(con = file[2])
  # the MITM appends a line containing "[LXC-Auth]" when an attacker enters the container; there will be one such line per file
  stime <- as.POSIXct(strsplit(
    Filter(
      function(x) {
        return (str_detect(x, "\\[LXC-Auth\\]"))
      },
      log)[1],
    " - ")[[1]][1])
  # the end time is simply the timestamp of the last line in the file
  etime <- as.POSIXct(strsplit(
    tail(log, 1)[1],
    " - ")[[1]][1])
  elapsed <- as.double(difftime(etime, stime, units="secs"))
  
  # for every attempt entry, find the difference between its time and the time associated with the MITM authentication
  closest_time <- rbindlist(
    apply(file_df, 1, function(x) {
      return(data.frame(Time=x[1], Difference=abs(difftime(x[1], stime, units="secs"))))
    }))
  # the closest time is the one with the minimum difference
  closest_time <- closest_time %>%
          slice(which.min(Difference))
  
  # if the time for any given row equals closest_time, that's the attempt that authenticated
  file_df <- as_tibble(file_df) %>%
    mutate(Authenticated=(Time==closest_time[1,1])) %>%
    mutate(Duration=ifelse(Authenticated==TRUE, elapsed, NA))
  return(file_df)
}
```

We pass our files into the methods we've just created and are treated with a neatly formatted table containing data about all 60,478 login attempts. The first 10 entries are below:

``` {r creation}
# join all of the data frames produced by calling parseFile on every pair of files into a single frame
attempt_df <- rbindlist(apply(files, 1, parseFile))
attempt_df %>%
  head(10) %>%
  kable(format = "html") %>%
  kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "500px")
```

## Exploratory Data Analysis
What's the point in having data if you don't do any analysis? The answer is there is none. We have an incredibly interesting data set at our disposal, and we would like to draw some equally interesting conclusions. Let's try to figure out what kind of information we can draw from our data. Here's what we have:
Time, IP, Username, Password, Country, Authenticated, Duration (seconds)

Here are some simple questions we might like to answer?
1. Which IPs contributed to the most attacks?
2. Which countries contributed to the most attacks?
3. What is the average amount of time spent in the honeypot, and what is the standard deviation of this duration?
4. What are the most common usernames/passwords attempted?
5. During what hour of the day to the most attacks occur?
6. Is attack duration normally distributed?

We will attempt to answer some of these questions, but first, we'll want to get everything set up. We import pandas and the plotting library, create a pandas DataFrame, and get the total number of entries in the dataframe below.

``` {python}
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(r.attempt_df)
total = len(df)

```

In the next code block, we will seek to answer the first question (which IPs performed the greatest number of attacks?) by doing a group by on IP address, and an aggregation on the number of entries. We will also add a column for the total proportion of attacks made up by that address. This is done using the count aggregate and the total entries determined previously.

``` {python}
countbyaddress = df.groupby(['Address', 'country_name']).count().reset_index()[['Address', 'country_name', 'Method']]
countbyaddress.columns = ['Address', 'Origin_Country', 'Count']
countbyaddress = countbyaddress.sort_values('Count', ascending=False).reset_index(drop=True).dropna()
countbyaddress['Proportion'] = countbyaddress['Count']/total
print('Attack count by IP Address:\n')
countbyaddress
```

We can see that the IP with the greatest number of attacks was from China and performed 2977 total attacks, or 5% of the total number of attacks.
Next, we will seek to answer question 2 by performing a similar series of operations as above, using the country name in place of the IP address. We will also add in a column to show the number of authenticated attacks and the authentication rate, since not every attempted attack is authenticated. We would expect that the authentication rate is pretty much constant given the way authentication is determined.

``` {python}
countbycountry = df.groupby(['country_name']).count().reset_index()[['country_name', 'Username']]
countbycountry.columns = ['Country', 'Count']
countbycountry = countbycountry.sort_values('Count', ascending=False).reset_index(drop=True)
countbycountry['Attack_Proportion'] = countbyaddress['Count']/total

successbycountry = df.groupby('country_name')['Authenticated'].apply(lambda x: (x==True).sum()).reset_index(name='count')
successbycountry.columns = ['Country', 'Authenticated']
countbycountry = countbycountry.join(successbycountry.set_index('Country'), on='Country')

countbycountry['Auth_Rate'] = countbycountry['Authenticated'] / countbycountry['Count']

countbycountry
```

The country with the most attacks is China, with nearly 50% of the total number of attacks at almost 21000. As we expect, the authentication rate is nearly constant, though naturally, the variation in the rate is much higher when the attempted attacks is very low.
To answer the third question (average duration and standard deviation) we will want to perform a simple aggregation on the data, as below.

``` {python}
mean_duration = df['Duration'].mean()
std_duration = df['Duration'].std()

print('mean duration = {}\nstandard deviation = {}'.format(mean_duration, std_duration))
```

We can see that the average duration is about 3 minutes and 14 seconds, while the standard deviation is much higher at 34 minutes and 43 seconds. This means that most attacks were relatively short, while a few attacks were much, much longer. Analyzing data with wildly different scales can be rather difficult, and we will see later that standardizing our data can make analysis much more useful.

Let's now order the countries by the average duration of their attacks.

``` {python}
duration = df.groupby(['country_name'])[['country_name', 'Duration']].mean().reset_index()
duration = duration.sort_values('Duration', ascending=False).dropna().reset_index(drop=True)
duration.columns = ['Country', 'Avg_Duration']

duration
```

As predicted earlier, we do indeed have a huge range in mean attack durations, from under 1 second to almost 1.5 hours. Let's plot this to see what we find.

``` {python}
duration['Rank'] = range(1, len(duration) + 1)
ax = duration.plot(x='Rank', y='Avg_Duration', color='black')
duration.plot(ax=ax, kind='scatter', x='Rank', y='Avg_Duration', color='black')
plt.show()
```

We can somewhat clearly tell that the average duration for the first few highest countries, but once we get past perhaps the top 10 countries, we barely tell the difference between them, and the scale of the y-axis does not help.
Repeating the same thing for all attacks only makes the problem worse.

``` {python}
duration_all = df.dropna().sort_values('Duration', ascending=False).reset_index(drop=True)
duration_all['Rank'] = range(1, len(duration_all) + 1)
duration_all = duration_all[['Rank', 'Duration']]

duration_all.plot(kind='scatter', x='Rank', y='Duration', color='black')
plt.show()
```

Again, there is no clear way to understand what each of the points mean. In order to make this data more understandable, we will have to standardize our data. We can do so by subtracting the mean value from every data point, so everything is centered around 0, and then dividing by the standard deviation, so every data point is listed in terms of how it's related to the other points.

``` {python}
mean = duration_all['Duration'].mean()
std = duration_all['Duration'].std()
duration_all['std_Duration'] = (duration_all['Duration'] - mean)/std

duration_all.plot(kind='scatter', x='Rank', y='std_Duration', color='black')
plt.show()
```

While the graph looks nearly identical to our previous graph, make no mistake - this one gives us much more information. Looking at the scale on the y-axis, we see that it labels the number of standard deviations above the mean for each point. This means that we can understand where each point is compared to one another. The first few points are many standard deviations above the other points, while the remainder of the points are at a very slightly negative standard deviation.
If we look at a histogram of the data, we see a pretty similar situation.

```{python}
duration_all['std_Duration'].plot.hist(bins=100)
plt.show()
```

In a normal distribution, we would expect the data to be centered and symmetrical around 0. Here, we clearly don't get that. This shows once again that our data is very far from normally distributed.

## Machine Learning

"This is where the fun begins." Now that we have compiled our data and looked at some initial trends in the data, it's time to put on our witch hats and look into the crystal ball— it's predicting time! 

To review, the attributes available to us are: Time, IP, Username, Password, Country, Authenticated, Duration (seconds).
With this, we can start to look at some factors that can be predicted. One of the key factors to know about any sort of attack in the place of origin. We are aware that an attacker can mask their location, using a VPN or other tools, much such blasphemy is beyond the scope of this project.

Username and passwords can vary too much; therefore, they will likely not contribute greatly towards pinpointing the origin of an attack. Due to the honeypots randomly authenticate attackers regardless of the credentials also rules out authentication as a possible predictor. IP is a direct predictor for location, making this problem trivial; therefore, we are left with Time and Duration as our predictors for Location.

Firstly, we have a bit of data cleansing to do. We drop any entries with NaN. Then, since country_name is a categorical variable, we use one-hot-encoding in order to turn it into a numeric variable which is now represented by a vector of 0s and a 1, where the position of the 1 describes that entity's country of origin.

Datetime is currently stored as a string, which poses our next problem. We turn this into a set of numeric variables by parsing into a pandas datetiem object then splitting year, month, day, hour, minute, and second into their respective columns.
```{python}
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

data = df[['Time','Duration','country_name']].dropna()
countries = data['country_name'].unique()
y = data.apply(lambda x: np.where(countries==x['country_name'])[0][0], axis=1).to_numpy()
y = preprocessing.label_binarize(y, classes=np.arange(len(countries)))

X = data.drop(['country_name'],axis=1)
X['Time'] = X.apply(lambda x: pd.to_datetime(x['Time']), axis=1)
X['year'] = X.apply(lambda x: x['Time'].year, axis=1)
X['month'] = X.apply(lambda x: x['Time'].month, axis=1)
X['day'] = X.apply(lambda x: x['Time'].day, axis=1)
X['hour'] = X.apply(lambda x: x['Time'].hour, axis=1)
X['minute'] = X.apply(lambda x: x['Time'].minute, axis=1)
X['second'] = X.apply(lambda x: x['Time'].second, axis=1)
X = X.drop(['Time'],axis=1).iloc[:,:].to_numpy()
X
y
```

Now we are ready for the next step: set up model and training. Since we have so many countries (87 of them to be specific), we need a multi-class classifier, making the One vs Rest Classifier the perfect fit. The One vs Rest Classifier selects a class to be positive and all others to be negative, then trains the classifier, repeating this n times for n different classifiers— one for each class.

We will be using 10% of our data to test, and the other 90% to train. Subsequently, we train our classifier(s) on our training set and test in order to obtain the testing score.
```{python}
from sklearn.model_selection import train_test_split
from sklearn import svm, datasets
from sklearn.multiclass import OneVsRestClassifier

# Learn to predict each class against the other
c = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,random_state=0))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0)
y_score = c.fit(X_train, y_train).decision_function(X_test)
```

From the scores we gathered, we can now calculate the ROC area which essentially measures the positive rate against the false positive rates in order to gauge how well the model can class balance— or how well we can distinguish one class from another.

To see the performance of our model, we will compute the micro averages which are computed by aggregating the contributions for each class then compute the ROC values. 
```{python}
from sklearn.metrics import roc_curve, auc

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(len(countries)):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

```

Once we have gathered our ROC data, we can now proceed to plot the ROC area data.
```{python}
import matplotlib.pyplot as plt
plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC area')
plt.legend(loc="lower right")
plt.show()
```

From this, we observe that we a greater than 1 ratio of true positive to false positive rates— looking good. The accuracy of our model is demonstrated by how much the ROC area curve is to the top left of the graph. Overall, we seem to be doing okay, but nothing ground breaking. We've been staring at the crystal ball for too long.

Because there are currently 87 different classes that are being summarized in the graph above, we want to see how the model did for a select few classes— five for example's sake. We will be looking at the individual ROC area curve for each of these five classes and their macro averages.

The macro average is computed by calculating the ROC independently for each class, then taking the average of all the ROCs (giving equal importance to all classes).
```{python}
from scipy import interp
# Aggregate all false positive rates
n_classes = len(countries[:5])
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

# Average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
fig = plt.figure(num=1, figsize=(10,10), dpi=80, facecolor='w', edgecolor='k')
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], lw=lw,
             label='ROC curve: {0} (area = {1:0.2f})'
             ''.format(countries[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Extension of Receiver operating characteristic to multi-class')
plt.legend(loc='upper left',fancybox=True, shadow=True, ncol=1)
plt.show()
```

Since there is such a huge imbalance between our classes (more data for one class than another), we see that our model is better at predicting one origin location than another. Particularly, we are ridiculously good at predicting attackers from Sweden— we got you! Not really, we likely have an extremely low subset of attacks originating from Sweden. The class imbalance is further demonstrated by the discrepancy between the micro and macro averages. We seem to be extremely good at predict some classes over the others, evident by our macro averages being mostly above 1.0. However, when we look at our micro average, we see that it is brought down due to this phenomena.

As we can see, life of a predictor is pretty hard. We have to deal with messy data, convoluted parsing, and huge class imbalances. Given these factors, we saw some very interesting resulting for the outcomes of each classes and how a few key attributes/ predictors can help us uncover unseen details, make predictions, and save the world from Armageddon.

The end.