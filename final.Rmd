---
title: "CMSC320 Final Project"
output: html_document
---

### Project by Nikos Koutsoheras, Sahil Dev, and Vivek Banerjee

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(reticulate)
library(rgeolocate)
library(stringr)
library(dplyr)

matplotlib <- import("matplotlib")
pandas <- import("pandas")
matplotlib$use("Agg", force = TRUE)
sklearn <- import("sklearn")
```

## Introduction
The days when systems administrators could rely on the obscurity of their servers as protection from poor configuration are behind us. Any machine connected to the Internet will be invariably subjected to a background radiation of attackers trying a variety of known software vulnerabilities and common security pitfalls in the hopes of stumbling upon a machine they can control. Only by understanding how these attacks work can we adequately protect our computers from them, and only by observing attackers can we understand the patterns governing their behaviors. 

Possibly the most effective way to observe an attacker is to watch them work on a machine we control. To this end, we construct a series of honeypots: servers made vulnerable to exploitation exposed on the open web. Attackers can freely connect to these honeypots, which will intentionally succumb to any exploit they attempt to use to gain access. Once inside, attackers can install any variety of malicious software they please and search for valuable data anywhere on the machine. When they leave, we have the opportunity to inspect their actions as well as any software they installed at our leisure.

The actual observation is carried out with a program known as a man-in-the-middle, which serves as the interface between the honeypot and the Internet, passing data seamlessly between the two while recording all that it relays.
(In reality, our honeypots are containers (a lightweight alternative to virtual machines that relies on services provided by the host operating system, rather than a virtualization of its own) running on the MITM server, but it may be easier to think of them as separate physical machines - they certainly could be.)

We analyze in this study the activity recorded on a configuration of five such honeypots managed by a single MITM server over the course of roughly two months.

The data consists of approximately 60,000 login attempts and 1500 attack sessions split across 3000 files.
Each most deeply nested folder represents a single instance of a single container. An instance folder will contain, among others, an attempts.txt file and a mitmlog_30*.txt file. We'll let R find them all and load them in.
```{r ingestion}
# locate all of the relevant files
# attempts contains tidy login data: each line contains data about a unique login attempt
attempts <- list.files(pattern = "attempts", recursive = TRUE)
# mitmlogs contains the raw logs generated by the MITM
mitmlogs <- list.files(pattern = "mitmlog", recursive = TRUE)
files <- data.frame(Attempts=attempts, Logs=mitmlogs)

# the binary database used by ip2location
ip_database <- system.file("extdata","GeoLite2-Country.mmdb", package = "rgeolocate")
```

The simpler of the two parsing steps involves the attempts.txt file present in every instance folder. The MITM manager script appended a line to this file every time somebody attempted to log in to the server. A single line contains the time of an attempted login, the IP address of the computer attemping to log in, the method they used (almost exclusively "password"), and the credentials they provided. We extract all of these fields and use the rgeolocate package to obtain the attacker's country of origin (or their VPN's) from their IP address.

Storing an attacker's IP and their country in every row represents a bit of redundancy. Ideally, we would store the IPs in the attempts table and create another table associating IPs with their corresponding countries. Our table isn't even in first normal form!
...But this is 320, not 424. We'll worry about that some other time.

``` {r parse line}
# have patience: this next step took about 88 seconds on my computer
# parses a single line in an attempts file
parseLine <- function(line) {
  # I'm lucky none of the attackers used a semicolon in their password guess; this could've been irritating
  segments <- strsplit(line, ";")
  location <- maxmind(segments[[1]][2], ip_database)
  return(data.frame(Time=segments[[1]][1],
                    Address=segments[[1]][2],
                    Method=segments[[1]][3],
                    Username=segments[[1]][4],
                    Password=segments[[1]][5],
                    CountryCode=location[3][1],
                    CountryName=location[2][1]))
}
```

In addition to the data we've retrieved from the attempts file, we also want to know how long an attacker spent in a honeypot. It would have been trivial to add a variable that kept track of that to the MITM manager script and write the difference to the end of every line in the attempts.txt files. Accessing it here could have been as simple as adding another field to the data frame we created in the parseLine method.
We lacked that insight at the time, so, as punishment, we must now parse the data we need from the larger log file.

Fortunately, the MITM log contains an entry for a successful authentication, which we can locate by the unique "[LXC-Auth]" marker that will appear once in every file. We also know that the container is shut down, destroyed, and rebuilt from scratch when the attacker leaves (or is forced out). As such, the time associated with the last line in the log entry represents the last point at which the attacker was active in the honeypot.

The timestamp attached to the authentication line in the MITM log will be a few milliseconds off from the corresponding entry in attempts.txt. To resolve the pair, we say that the authentication entry observed in the MITM log represents the same attempt as the line in the attempts file with the closest timestamp.
(Even though every IP address except for the attacker's is blacklisted from the container after a successful authentication, we can't just associate the successful entry with the last line in the attempts file because the attacker will occasionally try another username/password combination after gaining access. We speculate that the scripts the attackers use are programmed to try all of their credentials as quickly as possible, rather than wait for the server's response after each one.)

``` {r parse folder}
# parses a single attempts/mitmlog pair
parseFile <- function(file) {
  # join all of the one-row data frames produced by calling parseLine on every line in the attempts file into a single frame
  file_df <- rbindlist(lapply(readLines(con = file[1]), parseLine))
  
  log <- readLines(con = file[2])
  # the MITM appends a line containing "[LXC-Auth]" when an attacker enters the container; there will be one such line per file
  stime <- as.POSIXct(strsplit(
    Filter(
      function(x) {
        return (str_detect(x, "\\[LXC-Auth\\]"))
      },
      log)[1],
    " - ")[[1]][1])
  # the end time is simply the timestamp of the last line in the file
  etime <- as.POSIXct(strsplit(
    tail(log, 1)[1],
    " - ")[[1]][1])
  elapsed <- as.double(difftime(etime, stime, units="secs"))
  
  # for every attempt entry, find the difference between its time and the time associated with the MITM authentication
  closest_time <- rbindlist(
    apply(file_df, 1, function(x) {
      return(data.frame(Time=x[1], Difference=abs(difftime(x[1], stime, units="secs"))))
    }))
  # the closest time is the one with the minimum difference
  closest_time <- closest_time %>%
          slice(which.min(Difference))
  
  # if the time for any given row equals closest_time, that's the attempt that authenticated
  file_df <- as_tibble(file_df) %>%
    mutate(Authenticated=(Time==closest_time[1,1])) %>%
    mutate(Duration=ifelse(Authenticated==TRUE, elapsed, NA))
  return(file_df)
}

# join all of the data frames produced by calling parseFile on every pair of files into a single frame
attempt_df <- rbindlist(apply(files, 1, parseFile))
attempt_df
```

## Exploratory Data Analysis
What's the point in having data if you don't do any analysis? The answer is there is none. We have an incredibly interesting dataset at our disposal, and we would like to draw some equally interesting conclusions. Let's try to figure out what kind of information we can draw from our data. Here's what we have:
Time, IP, Username, Password, Country, Authenticated, Duration (seconds)

Here are some simple questions we might like to answer?
1. Which IPs contributed to the most attacks?
2. Which countries contributed to the most attacks?
3. What is the average amount of time spent in the honeypot, and what is the standard deviation of this duration?
4. What are the most common usernames/passwords attempted?
5. During what hour of the day to the most attacks occur?

We will attempt to answer some of these questions, but first, we'll want to get everything set up. We import pandas and the plotting library, create a pandas DataFrame, and get the total number of entries in the dataframe below.

``` {python}
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(r.attempt_df)
total = len(df)

```

In the next code block, we will seek to answer the first question (which IPs performed the greatest number of attacks?) by doing a group by on IP address, and an aggregation on the number of entries. We will also add a column for the total proportion of attacks made up by that address. This is done using the count aggregate and the total entries determined previously.

``` {python}
countbyaddress = df.groupby(['Address', 'country_name']).count().reset_index()[['Address', 'country_name', 'Method']]
countbyaddress.columns = ['Address', 'Origin_Country', 'Count']
countbyaddress = countbyaddress.sort_values('Count', ascending=False).reset_index(drop=True).dropna()
countbyaddress['Proportion'] = countbyaddress['Count']/total
print('Attack count by IP Address:\n')
countbyaddress
```

We can see that the IP with the greatest number of attacks was from China and performed 2977 total attacks, or 5% of the total number of attacks.
Next, we will seek to answer question 2 by performing a similar series of operations as above, using the country name in place of the IP address. We will also add in a column to show the number of authenticated attacks and the authentication rate, since not every attempted attack is authenticated. We would expect that the authentication rate is pretty much constant given the way authentication is determined.

``` {python}
countbycountry = df.groupby(['country_name']).count().reset_index()[['country_name', 'Username']]
countbycountry.columns = ['Country', 'Count']
countbycountry = countbycountry.sort_values('Count', ascending=False).reset_index(drop=True)
countbycountry['Attack_Proportion'] = countbyaddress['Count']/total

successbycountry = df.groupby('country_name')['Authenticated'].apply(lambda x: (x==True).sum()).reset_index(name='count')
successbycountry.columns = ['Country', 'Authenticated']
countbycountry = countbycountry.join(successbycountry.set_index('Country'), on='Country')

countbycountry['Auth_Rate'] = countbycountry['Authenticated'] / countbycountry['Count']

countbycountry
```

The country with the most attacks is China, with nearly 50% of the total number of attacks at almost 21000. As we expect, the authentication rate is nearly constant, though naturally, the variation in the rate is much higher when the attempted attacks is very low.
To answer the third question (average duration and standard deviation) we will want to perform a simple aggregation on the data, as below.

``` {python}
mean_duration = df['Duration'].mean()
std_duration = df['Duration'].std()

print('mean duration = {}\nstandard deviation = {}'.format(mean_duration, std_duration))
```

We can see that the average duration is about 3 minutes and 14 seconds, while the standard deviation is much higher at 34 minutes and 43 seconds. This means that most attacks were relatively short, while a few attacks were much, much longer. Analyzing data with wildly different scales can be rather difficult, and we will see later that standardizing our data can make analysis much more useful.

Let's now order the countries by the average duration of their attacks.

``` {python}
duration = df.groupby(['country_name'])[['country_name', 'Duration']].mean().reset_index()
duration = duration.sort_values('Duration', ascending=False).dropna().reset_index(drop=True)
duration.columns = ['Country', 'Avg_Duration']

duration
```

As predicted earlier, we do indeed have a huge range in mean attack durations, from under 1 second to almost 1.5 hours. Let's plot this to see what we find.

``` {python}
duration['Rank'] = range(1, len(duration) + 1)
ax = duration.plot(x='Rank', y='Avg_Duration', color='black')
duration.plot(ax=ax, kind='scatter', x='Rank', y='Avg_Duration', color='black')
plt.show()
```

We can somewhat clearly tell that the average duration for the first few highest countries, but once we get past perhaps the top 10 countries, we barely tell the difference between them, and the scale of the y-axis does not help.
Repeating the same thing for all attacks only makes the problem worse.

``` {python}
duration_all = df.dropna().sort_values('Duration', ascending=False).reset_index(drop=True)
duration_all['Rank'] = range(1, len(duration_all) + 1)
duration_all = duration_all[['Rank', 'Duration']]

duration_all.plot(kind='scatter', x='Rank', y='Duration', color='black')
plt.show()
```

Again, there is no clear way to understand what each of the points mean. In order to make this data more understandable, we will have to standardize our data. We can do so by subtracting the mean value from every data point, so everything is centered around 0, and then dividing by the standard deviation, so every data point is listed in terms of how it's related to the other points.

``` {python}
mean = duration_all['Duration'].mean()
std = duration_all['Duration'].std()
duration_all['std_Duration'] = (duration_all['Duration'] - mean)/std

duration_all.plot(kind='scatter', x='Rank', y='std_Duration', color='black')
plt.show()
```

While the graph looks nearly identical to our previous graph, make no mistake - this one gives us much more information. Looking at the scale on the y-axis, we see that it labels the number of standard deviations above the mean for each point. This means that we can understand where each point is compared to one another. The first few points are many standard deviations above the other points, while the remainder of the points are at a very slightly negative standard deviation.
If we look at a histogram of the data, we see a pretty similar situation.

```{python}
duration_all['std_Duration'].plot.hist(bins=100)
plt.show()
```

In a normal distribution, we would expect the data to be centered and symmetrical around 0. Here, we clearly don't get that. This shows once again that our data is very far from normally distributed.

## ML and stuff

We will use ToD and duration to predict the location of an attack.To start, we will do some data preparation by encoding the country name as decimal values.
```{python}
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

data = df[['Time','Duration','country_name']].dropna()
countries = data['country_name'].unique()
y = data.apply(lambda x: np.where(countries==x['country_name'])[0][0], axis=1).to_numpy()
y = preprocessing.label_binarize(y, classes=np.arange(len(countries)))

X = data.drop(['country_name'],axis=1)
X['Time'] = X.apply(lambda x: pd.to_datetime(x['Time']), axis=1)
X['year'] = X.apply(lambda x: x['Time'].year, axis=1)
X['month'] = X.apply(lambda x: x['Time'].month, axis=1)
X['day'] = X.apply(lambda x: x['Time'].day, axis=1)
X['hour'] = X.apply(lambda x: x['Time'].hour, axis=1)
X['minute'] = X.apply(lambda x: x['Time'].minute, axis=1)
X['second'] = X.apply(lambda x: x['Time'].second, axis=1)
X = X.drop(['Time'],axis=1).iloc[:,:].to_numpy()
X
y
```

Set up models.
```{python}
import sklearn.ensemble
import sklearn.model_selection
import sklearn.metrics
import sklearn.utils.multiclass
from sklearn.model_selection import KFold

#des_tree = sklearn.tree.DecisionTreeClassifier(max_depth=None, min_samples_split=2)
#param_des = {'max_features': ['auto']}

#tree_cv = sklearn.model_selection.GridSearchCV(des_tree, param_des, cv=10)
#cv_obj = sklearn.model_selection.StratifiedKFold(n_splits=10)
#cv_obj = KFold(n_splits=10)

#for i, (train, test) in enumerate(cv_obj.split(X, y)):
#   tree_cv.fit(X[train], y[train])
#   scores = tree_cv.predict_proba(X[test])
#   print(scores)

def get_roc_data(model, cv_obj, Xi, yi):
    curve_df = None
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)
    
    for i, (train, test) in enumerate(cv_obj.split(Xi, yi)):
        model.fit(Xi[train], yi[train])
        scores = model.predict_proba(Xi[test])
        fpr, tpr, _ = sklearn.metrics.roc_curve(yi[test],scores)
        
        interp_tpr = np.interp(mean_fpr, fpr, tpr)
        interp_tpr[0] = 0.0
        tmp = pd.DataFrame({'fold':i, 'fpr': mean_fpr, 'tpr': interp_tpr})
        curve_df = tmp if curve_df is None else pd.concat([curve_df, tmp])
        
        aucs.append(sklearn.metrics.auc(fpr, tpr))
        
    curve_df = curve_df.groupby('fpr').agg({'tpr': 'mean'}).reset_index()
    curve_df.iloc[-1,1] = 1.0
    
    auc_df = pd.DataFrame({'fold': np.arange(len(aucs)), 'auc': aucs})
        
    return 0,0
    return curve_df, auc_df
    
        
```

Let the learning begin!
```{python}
#tree_curve_df, tree_auc_df = get_roc_data(tree_cv, cv_obj, X, y)
#tree_auc_df
```